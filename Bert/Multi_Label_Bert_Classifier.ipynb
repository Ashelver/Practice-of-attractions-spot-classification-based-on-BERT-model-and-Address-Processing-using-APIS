{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XL-qzYyqmLrC",
        "outputId": "9ff565d3-8c28-462b-b538-7c5d2abe37cb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "标签数量: 9\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers\n",
        "import json\n",
        "import numpy as np\n",
        "\n",
        "# 读取训练数据 (Read training data)\n",
        "with open('训练数据路径', 'r', encoding='utf-8') as f:\n",
        "    train_data = json.load(f)\n",
        "\n",
        "# 读取验证数据 (Read validation data)\n",
        "with open('验证数据路径', 'r', encoding='utf-8') as f:\n",
        "    validation_data = json.load(f)\n",
        "\n",
        "# 将标签转换为0和1整数列表 (Convert labels to a list of integers 0 and 1)\n",
        "def convert_labels(data):\n",
        "    for item in data:\n",
        "        labels = item['label']\n",
        "        label_list = np.zeros(len(all_labels))  # 假设有all_labels个标签 (Assume there are all_labels labels)\n",
        "        for label in labels:\n",
        "            if label in all_labels:\n",
        "                label_list[all_labels.index(label)] = 1\n",
        "        item['label'] = label_list.tolist()\n",
        "\n",
        "# 获取所有可能的标签 (Get all possible labels)\n",
        "all_labels = []\n",
        "for item in train_data + validation_data:\n",
        "    all_labels.extend(item['label'])\n",
        "all_labels = list(set(all_labels))\n",
        "\n",
        "# 转换训练数据和验证数据的标签 (Convert labels of training and validation data)\n",
        "convert_labels(train_data)\n",
        "convert_labels(validation_data)\n",
        "print(\"标签数量: \" + str(len(all_labels)))  # (Number of labels)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8qeq903nmMvB",
        "outputId": "29e370e0-a9b2-4a11-d62e-0f49205c6e24"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.33.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.2)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.15.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.17.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.13.3)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.3.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-chinese and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "from transformers import BertTokenizer, BertForSequenceClassification\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "\n",
        "# 加载Bert预训练模型和分词器 (Load Bert pre-trained model and tokenizer)\n",
        "model_name = 'bert-base-chinese'  # 或 'bert-base-uncased'（英文）(or 'bert-base-uncased' for English)\n",
        "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
        "model = BertForSequenceClassification.from_pretrained(model_name, num_labels=len(all_labels))\n",
        "\n",
        "# 自定义数据集类 (Custom dataset class)\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, data):\n",
        "        self.data = data\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = self.data[idx]\n",
        "        text = item['text']\n",
        "        label = item['label']\n",
        "        encoded_input = tokenizer.encode_plus(\n",
        "            text,\n",
        "            add_special_tokens=True,\n",
        "            max_length=512,\n",
        "            padding='max_length',\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "        return {\n",
        "            'input_ids': encoded_input['input_ids'].squeeze(),\n",
        "            'attention_mask': encoded_input['attention_mask'].squeeze(),\n",
        "            'label': torch.tensor(label, dtype=torch.float)\n",
        "        }\n",
        "\n",
        "# 创建训练集和验证集的数据加载器 (Create data loaders for training and validation sets)\n",
        "train_dataset = CustomDataset(train_data)\n",
        "train_loader = DataLoader(train_dataset, batch_size=len(all_labels), shuffle=True)\n",
        "\n",
        "validation_dataset = CustomDataset(validation_data)\n",
        "validation_loader = DataLoader(validation_dataset, batch_size=len(all_labels), shuffle=False)\n",
        "\n",
        "# 将模型移动到GPU（如果可用） (Move model to GPU if available)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model.to(device)\n",
        "\n",
        "# 定义训练函数 (Define training function)\n",
        "def train(model, train_loader, validation_loader, device, num_epochs, threshold=0):\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\n",
        "    # criterion = torch.nn.BCEWithLogitsLoss()\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        train_loss = 0.0\n",
        "\n",
        "        train_results = []\n",
        "        for batch in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            labels = batch['label'].to(device)\n",
        "\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "            loss = outputs.loss\n",
        "            logits = outputs.logits\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            train_loss += loss.item()\n",
        "            for i in range(len(logits)):\n",
        "                result = {\n",
        "                    'logits': logits[i].detach().cpu().numpy().tolist(),\n",
        "                    'labels': labels[i].detach().cpu().numpy().tolist()\n",
        "                }\n",
        "                train_results.append(result)\n",
        "\n",
        "        # 验证模型 (Validate model)\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        predictions = []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            valid_results = []\n",
        "            for batch in validation_loader:\n",
        "                input_ids = batch['input_ids'].to(device)\n",
        "                attention_mask = batch['attention_mask'].to(device)\n",
        "                labels = batch['label'].to(device)\n",
        "\n",
        "                outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "                loss = outputs.loss\n",
        "                logits = outputs.logits\n",
        "\n",
        "                val_loss += loss.item()\n",
        "                predictions.extend(logits.sigmoid().cpu().numpy())\n",
        "                for i in range(len(logits)):\n",
        "                    result = {\n",
        "                        'logits': logits[i].detach().cpu().numpy().tolist(),\n",
        "                        'labels': labels[i].detach().cpu().numpy().tolist()\n",
        "                    }\n",
        "                    valid_results.append(result)\n",
        "        train_loss /= len(train_loader)\n",
        "        val_loss /= len(validation_loader)\n",
        "\n",
        "        print(f'Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}')\n",
        "\n",
        "\n",
        "        # 将结果保存到json文件 (Save results to JSON file)\n",
        "        with open('训练集的结果保存路径', 'w', encoding='utf-8') as f:  # ('Path to save training set results')\n",
        "            json.dump(train_results, f)\n",
        "\n",
        "        with open('验证集的结果保存路径', 'w', encoding='utf-8') as f:  # ('Path to save validation set results')\n",
        "            json.dump(valid_results, f)\n",
        "\n",
        "        # # 根据阈值生成预测 (Generate predictions based on threshold)\n",
        "        # thresholded_predictions = [[1 if prob >= threshold else 0 for prob in pred] for pred in predictions]\n",
        "        # print(f'Thresholded Predictions: {thresholded_predictions}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ziReYUikrV6A",
        "outputId": "b7b698c2-bfdc-471f-b1e4-bf346abdb592"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/30, Train Loss: 0.3251, Val Loss: 0.2436\n",
            "Epoch 2/30, Train Loss: 0.2215, Val Loss: 0.1845\n",
            "Epoch 3/30, Train Loss: 0.1662, Val Loss: 0.1593\n",
            "Epoch 4/30, Train Loss: 0.1249, Val Loss: 0.1553\n",
            "Epoch 5/30, Train Loss: 0.1021, Val Loss: 0.1699\n",
            "Epoch 6/30, Train Loss: 0.0886, Val Loss: 0.1648\n",
            "Epoch 7/30, Train Loss: 0.0653, Val Loss: 0.1629\n",
            "Epoch 8/30, Train Loss: 0.0559, Val Loss: 0.1774\n",
            "Epoch 9/30, Train Loss: 0.0514, Val Loss: 0.1677\n",
            "Epoch 10/30, Train Loss: 0.0469, Val Loss: 0.1749\n",
            "Epoch 11/30, Train Loss: 0.0391, Val Loss: 0.1896\n",
            "Epoch 12/30, Train Loss: 0.0339, Val Loss: 0.1841\n",
            "Epoch 13/30, Train Loss: 0.0347, Val Loss: 0.1954\n",
            "Epoch 14/30, Train Loss: 0.0269, Val Loss: 0.1848\n",
            "Epoch 15/30, Train Loss: 0.0268, Val Loss: 0.1961\n",
            "Epoch 16/30, Train Loss: 0.0258, Val Loss: 0.1959\n",
            "Epoch 17/30, Train Loss: 0.0241, Val Loss: 0.2265\n",
            "Epoch 18/30, Train Loss: 0.0245, Val Loss: 0.2115\n",
            "Epoch 19/30, Train Loss: 0.0225, Val Loss: 0.2119\n",
            "Epoch 20/30, Train Loss: 0.0250, Val Loss: 0.1976\n",
            "Epoch 21/30, Train Loss: 0.0252, Val Loss: 0.2095\n",
            "Epoch 22/30, Train Loss: 0.0206, Val Loss: 0.1995\n",
            "Epoch 23/30, Train Loss: 0.0184, Val Loss: 0.2087\n",
            "Epoch 24/30, Train Loss: 0.0170, Val Loss: 0.2092\n",
            "Epoch 25/30, Train Loss: 0.0159, Val Loss: 0.2081\n",
            "Epoch 26/30, Train Loss: 0.0146, Val Loss: 0.2275\n",
            "Epoch 27/30, Train Loss: 0.0154, Val Loss: 0.2271\n",
            "Epoch 28/30, Train Loss: 0.0143, Val Loss: 0.2259\n",
            "Epoch 29/30, Train Loss: 0.0293, Val Loss: 0.1926\n",
            "Epoch 30/30, Train Loss: 0.0207, Val Loss: 0.2154\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# 开始训练 (start training)\n",
        "train(model, train_loader, validation_loader, device, num_epochs=30, threshold=0.5)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
